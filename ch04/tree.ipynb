{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://912db3bbf5c4:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1611569726982)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, RandomForestClassificationModel}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import scala.util.Random\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier,\n",
    "  RandomForestClassifier, RandomForestClassificationModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataWithoutHeader = spark.read.\n",
    "      option(\"inferSchema\", true).\n",
    "      option(\"header\", false).\n",
    "      csv(\"./data/covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+----+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "| _c0|_c1|_c2|_c3|_c4| _c5|_c6|_c7|_c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|_c54|\n",
      "+----+---+---+---+---+----+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|2596| 51|  3|258|  0| 510|221|232|148|6279|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2590| 56|  2|212| -6| 390|220|235|151|6225|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2804|139|  9|268| 65|3180|234|238|135|6121|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|\n",
      "|2785|155| 18|242|118|3090|238|238|122|6211|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|\n",
      "|2595| 45|  2|153| -1| 391|220|234|150|6172|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2579|132|  6|300|-15|  67|230|237|140|6031|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|\n",
      "|2606| 45|  7|270|  5| 633|222|225|138|6256|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2605| 49|  4|234|  7| 573|222|230|144|6228|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2617| 45|  9|240| 56| 666|223|221|133|6244|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2612| 59| 10|247| 11| 636|228|219|124|6230|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2612|201|  4|180| 51| 735|218|243|161|6222|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2886|151| 11|371| 26|5253|234|240|136|4051|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|\n",
      "|2742|134| 22|150| 69|3215|248|224| 92|6091|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|\n",
      "|2609|214|  7|150| 46| 771|213|247|170|6211|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2503|157|  4| 67|  4| 674|224|240|151|5600|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2495| 51|  7| 42|  2| 752|224|225|137|5576|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2610|259|  1|120| -1| 607|216|239|161|6096|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2517| 72|  7| 85|  6| 595|228|227|133|5607|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2504|  0|  4| 95|  5| 691|214|232|156|5572|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "|2503| 38|  5| 85| 10| 741|220|228|144|5555|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "+----+---+---+---+---+----+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataWithoutHeader.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.Row = [2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataWithoutHeader.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colNames: Seq[String] = List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames = Seq(\n",
    "        \"Elevation\", \"Aspect\", \"Slope\",\n",
    "        \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "        \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\"\n",
    "      ) ++ (\n",
    "        (0 until 4).map(i => s\"Wilderness_Area_$i\")\n",
    "      ) ++ (\n",
    "        (0 until 40).map(i => s\"Soil_Type_$i\")\n",
    "      ) ++ Seq(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: scala.collection.immutable.IndexedSeq[String] = Vector(Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0 until 4).map(i => s\"Wilderness_Area_$i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLlib의 모든 api는 int가 아닌 Double만 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = dataWithoutHeader.toDF(colNames:_*).\n",
    "  withColumn(\"Cover_Type\", $\"Cover_Type\".cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "res3: testData.type = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData, testData) = data.randomSplit(Array(0.9, 0.1))\n",
    "trainData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. assembler : CSR 읽는 방법 체크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLlib 파이프라인 API중 Transformer의 예시\n",
    "- 데이터프레임을 다른 데이터프레임으로 변환\n",
    "- 이런 변환들을 묶어 하나의 파이프라인으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, S...\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputCols = trainData.columns.filter(_ != \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "  setInputCols(inputCols).\n",
    "  setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.sql.Row] = Array([1874,18,14,0,0,90,208,209,135,793,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6.0], [1879,28,19,30,12,95,209,196,117,778,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6.0], [1888,33,22,150,46,108,209,185,103,735,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6.0])\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                 |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0,209.0,196.0,117.0,778.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1888.0,33.0,22.0,150.0,46.0,108.0,209.0,185.0,103.0,735.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1899.0,355.0,22.0,153.0,43.0,124.0,178.0,195.0,151.0,819.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1903.0,67.0,16.0,108.0,36.0,120.0,234.0,207.0,100.0,969.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1904.0,51.0,26.0,67.0,30.0,162.0,222.0,175.0,72.0,711.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1905.0,19.0,27.0,134.0,58.0,120.0,188.0,171.0,108.0,636.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1905.0,33.0,27.0,90.0,46.0,150.0,204.0,171.0,89.0,725.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1905.0,77.0,21.0,90.0,38.0,120.0,241.0,196.0,75.0,1025.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1906.0,356.0,20.0,150.0,55.0,120.0,184.0,201.0,151.0,726.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1908.0,323.0,32.0,150.0,52.0,120.0,125.0,190.0,196.0,765.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[1918.0,321.0,28.0,42.0,17.0,85.0,139.0,201.0,196.0,402.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1919.0,30.0,22.0,67.0,9.0,256.0,208.0,188.0,107.0,661.0,1.0,1.0])   |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembledTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 54 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembledTrainData = assembler.transform(trainData)\n",
    "assembledTrainData.select(\"featureVector\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 54 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembledTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation\n",
      "Aspect\n",
      "Slope\n",
      "Horizontal_Distance_To_Hydrology\n",
      "Vertical_Distance_To_Hydrology\n",
      "Horizontal_Distance_To_Roadways\n",
      "Hillshade_9am\n",
      "Hillshade_Noon\n",
      "Hillshade_3pm\n",
      "Horizontal_Distance_To_Fire_Points\n",
      "Wilderness_Area_0\n",
      "Wilderness_Area_1\n",
      "Wilderness_Area_2\n",
      "Wilderness_Area_3\n",
      "Soil_Type_0\n",
      "Soil_Type_1\n",
      "Soil_Type_2\n",
      "Soil_Type_3\n",
      "Soil_Type_4\n",
      "Soil_Type_5\n",
      "Soil_Type_6\n",
      "Soil_Type_7\n",
      "Soil_Type_8\n",
      "Soil_Type_9\n",
      "Soil_Type_10\n",
      "Soil_Type_11\n",
      "Soil_Type_12\n",
      "Soil_Type_13\n",
      "Soil_Type_14\n",
      "Soil_Type_15\n",
      "Soil_Type_16\n",
      "Soil_Type_17\n",
      "Soil_Type_18\n",
      "Soil_Type_19\n",
      "Soil_Type_20\n",
      "Soil_Type_21\n",
      "Soil_Type_22\n",
      "Soil_Type_23\n",
      "Soil_Type_24\n",
      "Soil_Type_25\n",
      "Soil_Type_26\n",
      "Soil_Type_27\n",
      "Soil_Type_28\n",
      "Soil_Type_29\n",
      "Soil_Type_30\n",
      "Soil_Type_31\n",
      "Soil_Type_32\n",
      "Soil_Type_33\n",
      "Soil_Type_34\n",
      "Soil_Type_35\n",
      "Soil_Type_36\n",
      "Soil_Type_37\n",
      "Soil_Type_38\n",
      "Soil_Type_39\n",
      "Cover_Type\n",
      "featureVector\n"
     ]
    }
   ],
   "source": [
    "assembledTrainData.columns.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=dtc_3e08ba901394, depth=5, numNodes=47, numClasses=8, numFeatures=54\n",
      "  If (feature 0 <= 3048.5)\n",
      "   If (feature 0 <= 2541.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 0 <= 2408.5)\n",
      "      If (feature 3 <= 15.0)\n",
      "       Predict: 4.0\n",
      "      Else (feature 3 > 15.0)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2408.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     If (feature 9 <= 5472.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 9 > 5472.5)\n",
      "      If (feature 5 <= 542.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 542.0)\n",
      "       Predict: 5.0\n",
      "   Else (feature 0 > 2541.5)\n",
      "    If (feature 0 <= 2956.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 17 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 0 > 2956.5)\n",
      "     If (feature 3 <= 191.0)\n",
      "      If (feature 36 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 36 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 3 > 191.0)\n",
      "      Predict: 2.0\n",
      "  Else (feature 0 > 3048.5)\n",
      "   If (feature 0 <= 3316.5)\n",
      "    If (feature 7 <= 239.5)\n",
      "     If (feature 0 <= 3137.5)\n",
      "      If (feature 45 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 45 > 0.5)\n",
      "       Predict: 2.0\n",
      "     Else (feature 0 > 3137.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 7 > 239.5)\n",
      "     If (feature 3 <= 337.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 337.0)\n",
      "      If (feature 0 <= 3202.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3202.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3316.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 296.0)\n",
      "      If (feature 6 <= 206.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 206.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 296.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 1002.0)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 1002.0)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_3e08ba901394\n",
       "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel: uid=dtc_3e08ba901394, depth=5, numNodes=47, numClasses=8, numFeatures=54\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifier = new DecisionTreeClassifier().\n",
    "  setSeed(Random.nextLong()).\n",
    "  setLabelCol(\"Cover_Type\").\n",
    "  setFeaturesCol(\"featureVector\").\n",
    "  setPredictionCol(\"prediction\")\n",
    "\n",
    "val model = classifier.fit(assembledTrainData)\n",
    "println(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 0은 의미 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[org.apache.spark.sql.Row] = Array([7.0], [1.0], [4.0], [3.0], [2.0], [6.0], [5.0])\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembledTrainData.select(\"Cover_Type\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                       |\n",
      "+----------+----------+--------------------------------------------------------------------------------------------------+\n",
      "|6.0       |4.0       |[0.0,0.0,0.031062124248496994,0.29458917835671344,0.48396793587174347,0.0,0.1903807615230461,0.0] |\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.024144129422338425,0.6469482800882425,0.059686248876542204,0.0,0.26922134161287686,0.0]|\n",
      "+----------+----------+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 57 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(assembledTrainData)\n",
    "\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").\n",
    "show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8115508430225156,Elevation)\n",
      "(0.04176007601079249,Horizontal_Distance_To_Hydrology)\n",
      "(0.02699788134250767,Soil_Type_1)\n",
      "(0.02635013526940948,Soil_Type_3)\n",
      "(0.025116576702914076,Hillshade_Noon)\n",
      "(0.024676057165493523,Soil_Type_31)\n",
      "(0.02037603230938446,Wilderness_Area_0)\n",
      "(0.011826889934156499,Wilderness_Area_2)\n",
      "(0.005591085323383825,Soil_Type_22)\n",
      "(0.0025495624985275604,Horizontal_Distance_To_Roadways)\n",
      "(0.0025348532961281707,Hillshade_9am)\n",
      "(6.700071247868423E-4,Horizontal_Distance_To_Fire_Points)\n",
      "(0.0,Wilderness_Area_3)\n",
      "(0.0,Wilderness_Area_1)\n",
      "(0.0,Vertical_Distance_To_Hydrology)\n",
      "(0.0,Soil_Type_9)\n",
      "(0.0,Soil_Type_8)\n",
      "(0.0,Soil_Type_7)\n",
      "(0.0,Soil_Type_6)\n",
      "(0.0,Soil_Type_5)\n",
      "(0.0,Soil_Type_4)\n",
      "(0.0,Soil_Type_39)\n",
      "(0.0,Soil_Type_38)\n",
      "(0.0,Soil_Type_37)\n",
      "(0.0,Soil_Type_36)\n",
      "(0.0,Soil_Type_35)\n",
      "(0.0,Soil_Type_34)\n",
      "(0.0,Soil_Type_33)\n",
      "(0.0,Soil_Type_32)\n",
      "(0.0,Soil_Type_30)\n",
      "(0.0,Soil_Type_29)\n",
      "(0.0,Soil_Type_28)\n",
      "(0.0,Soil_Type_27)\n",
      "(0.0,Soil_Type_26)\n",
      "(0.0,Soil_Type_25)\n",
      "(0.0,Soil_Type_24)\n",
      "(0.0,Soil_Type_23)\n",
      "(0.0,Soil_Type_21)\n",
      "(0.0,Soil_Type_20)\n",
      "(0.0,Soil_Type_2)\n",
      "(0.0,Soil_Type_19)\n",
      "(0.0,Soil_Type_18)\n",
      "(0.0,Soil_Type_17)\n",
      "(0.0,Soil_Type_16)\n",
      "(0.0,Soil_Type_15)\n",
      "(0.0,Soil_Type_14)\n",
      "(0.0,Soil_Type_13)\n",
      "(0.0,Soil_Type_12)\n",
      "(0.0,Soil_Type_11)\n",
      "(0.0,Soil_Type_10)\n",
      "(0.0,Soil_Type_0)\n",
      "(0.0,Slope)\n",
      "(0.0,Hillshade_3pm)\n",
      "(0.0,Aspect)\n"
     ]
    }
   ],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).\n",
    "    sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_5464f6fcaca5, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "  setLabelCol(\"Cover_Type\").\n",
    "  setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.703284812473952\n",
      "0.6864919747345175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy: Double = 0.703284812473952\n",
       "f1: Double = 0.6864919747345175\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "val f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "println(accuracy)\n",
    "println(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix. 여전히 RDD만 지원하는듯? [Docs](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/MulticlassMetrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131309.0  54190.0   197.0    0.0    0.0   0.0  5046.0   \n",
      "50731.0   196235.0  7353.0   62.0   16.0  0.0  701.0    \n",
      "0.0       2557.0    29084.0  588.0  0.0   0.0  0.0      \n",
      "0.0       0.0       1505.0   966.0  0.0   0.0  0.0      \n",
      "4.0       7693.0    768.0    0.0    77.0  0.0  0.0      \n",
      "0.0       3365.0    11813.0  380.0  0.0   0.0  0.0      \n",
      "8161.0    15.0      59.0     0.0    0.0   0.0  10199.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictionRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[118] at rdd at <console>:40\n",
       "multiclassMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@193dd009\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionRDD = predictions.\n",
    "    select(\"prediction\", \"Cover_Type\").\n",
    "    as[(Double,Double)].rdd\n",
    "val multiclassMetrics = new MulticlassMetrics(predictionRDD)\n",
    "println(multiclassMetrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionDataset: org.apache.spark.sql.Dataset[(Double, Double)] = [prediction: double, Cover_Type: double]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 여기까지는 Dataset\n",
    "val predictionDataset = predictions.\n",
    "      select(\"prediction\", \"Cover_Type\").\n",
    "      as[(Double,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+---+---+---+-----+\n",
      "|Cover_Type|     1|     2|    3|  4|  5|  6|    7|\n",
      "+----------+------+------+-----+---+---+---+-----+\n",
      "|       1.0|131309| 54190|  197|  0|  0|  0| 5046|\n",
      "|       2.0| 50731|196235| 7353| 62| 16|  0|  701|\n",
      "|       3.0|     0|  2557|29084|588|  0|  0|    0|\n",
      "|       4.0|     0|     0| 1505|966|  0|  0|    0|\n",
      "|       5.0|     4|  7693|  768|  0| 77|  0|    0|\n",
      "|       6.0|     0|  3365|11813|380|  0|  0|    0|\n",
      "|       7.0|  8161|    15|   59|  0|  0|  0|10199|\n",
      "+----------+------+------+-----+---+---+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "confusionMatrix: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Cover_Type: double, 1: bigint ... 6 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val confusionMatrix = predictions.\n",
    "  groupBy(\"Cover_Type\").\n",
    "  pivot(\"prediction\", (1 to 7)).\n",
    "  count().\n",
    "  na.fill(0.0).\n",
    "  orderBy(\"Cover_Type\")\n",
    "\n",
    "confusionMatrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classProbabilities: (data: org.apache.spark.sql.DataFrame)Array[Double]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classProbabilities(data: DataFrame): Array[Double] = {\n",
    "    val total = data.count()\n",
    "    data.groupBy(\"Cover_Type\").count().\n",
    "    orderBy(\"Cover_Type\").\n",
    "    select(\"count\").as[Double].\n",
    "    map(_ / total).\n",
    "    collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainPriorProbabilities: Array[Double] = Array(0.36465586131216615, 0.4876900782680844, 0.061614609022815126, 0.004723996987041987, 0.01633038537568298, 0.02974340150724372, 0.035241667526965594)\n",
       "testPriorProbabilities: Array[Double] = Array(0.3641478822189237, 0.48677897062377024, 0.060840898891918946, 0.004763712934516207, 0.01641409782871345, 0.03122303151644862, 0.03583140598570886)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainPriorProbabilities = classProbabilities(trainData)\n",
    "val testPriorProbabilities = classProbabilities(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy: Double = 0.3764166120842551\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = trainPriorProbabilities.zip(testPriorProbabilities).map {\n",
    "  case (trainProb, cvProb) => trainProb * cvProb\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "41: error: missing parameter type",
     "output_type": "error",
     "traceback": [
      "<console>:41: error: missing parameter type",
      "Note: The expected type requires a one-argument function accepting a 2-Tuple.",
      "      Consider a pattern matching anonymous function, `{ case (trainProb, cvProb) =>  ... }`",
      "        (trainProb, cvProb) => trainProb * cvProb",
      "         ^",
      "<console>:41: error: missing parameter type",
      "        (trainProb, cvProb) => trainProb * cvProb",
      "                    ^",
      ""
     ]
    }
   ],
   "source": [
    "// case 없으면 에러남. https://jdm.kr/blog/85#멀티_바인딩(destructuring_bind)\n",
    "val accuracy = trainPriorProbabilities.zip(testPriorProbabilities).map {\n",
    " (trainProb, cvProb) => trainProb * cvProb\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, S...\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputCols = trainData.columns.filter(_ != \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "  setInputCols(inputCols).\n",
    "  setOutputCol(\"featureVector\")\n",
    "\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "  setSeed(Random.nextLong()).\n",
    "  setLabelCol(\"Cover_Type\").\n",
    "  setFeaturesCol(\"featureVector\").\n",
    "  setPredictionCol(\"prediction\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tdtc_7201084bac8d-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_7201084bac8d-maxDepth: 20\n",
       "})\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder().\n",
    "  addGrid(classifier.maxDepth, Seq(1, 20)).\n",
    "  build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiclassEval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_68449c92b7b6, metricName=accuracy, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "  setLabelCol(\"Cover_Type\").\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validator: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_f86e6a833f32\n",
       "validatorModel: org.apache.spark.ml.tuning.TrainValidationSplitModel = TrainValidationSplitModel: uid=tvs_f86e6a833f32, bestModel=pipeline_bcc309c6a2c0, trainRatio=0.9\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validator = new TrainValidationSplit().\n",
    "  setSeed(Random.nextLong()).\n",
    "  setEstimator(pipeline).  // 어떤 데이터, 모델을\n",
    "  setEvaluator(multiclassEval).  // 어떤 지표로\n",
    "  setEstimatorParamMaps(paramGrid).  // 어떤 하이퍼파라미터를\n",
    "  setTrainRatio(0.9)\n",
    "\n",
    "val validatorModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제일 좋은 모델 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033259848557462\n",
      "{\n",
      "\tdtc_7201084bac8d-maxDepth: 20\n",
      "}\n",
      "\n",
      "0.6296942394325697\n",
      "{\n",
      "\tdtc_7201084bac8d-maxDepth: 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramsAndMetrics: Array[(Double, org.apache.spark.ml.param.ParamMap)] =\n",
       "Array((0.9033259848557462,{\n",
       "\tdtc_7201084bac8d-maxDepth: 20\n",
       "}), (0.6296942394325697,{\n",
       "\tdtc_7201084bac8d-maxDepth: 1\n",
       "}))\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramsAndMetrics = validatorModel.validationMetrics.\n",
    "  zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)\n",
    "\n",
    "paramsAndMetrics.foreach { case (metric, params) =>\n",
    "    println(metric)\n",
    "    println(params)\n",
    "    println()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tdtc_7201084bac8d-cacheNodeIds: false,\n",
      "\tdtc_7201084bac8d-checkpointInterval: 10,\n",
      "\tdtc_7201084bac8d-featuresCol: featureVector,\n",
      "\tdtc_7201084bac8d-impurity: gini,\n",
      "\tdtc_7201084bac8d-labelCol: Cover_Type,\n",
      "\tdtc_7201084bac8d-leafCol: ,\n",
      "\tdtc_7201084bac8d-maxBins: 32,\n",
      "\tdtc_7201084bac8d-maxDepth: 20,\n",
      "\tdtc_7201084bac8d-maxMemoryInMB: 256,\n",
      "\tdtc_7201084bac8d-minInfoGain: 0.0,\n",
      "\tdtc_7201084bac8d-minInstancesPerNode: 1,\n",
      "\tdtc_7201084bac8d-minWeightFractionPerNode: 0.0,\n",
      "\tdtc_7201084bac8d-predictionCol: prediction,\n",
      "\tdtc_7201084bac8d-probabilityCol: probability,\n",
      "\tdtc_7201084bac8d-rawPredictionCol: rawPrediction,\n",
      "\tdtc_7201084bac8d-seed: -667345610833198024\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bestModel: org.apache.spark.ml.Model[_] = pipeline_bcc309c6a2c0\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bestModel = validatorModel.bestModel\n",
    "println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033259848557462\n"
     ]
    }
   ],
   "source": [
    "println(validatorModel.validationMetrics.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8998757292277952\n",
      "0.9354546393053373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testAccuracy: Double = 0.8998757292277952\n",
       "trainAccuracy: Double = 0.9354546393053373\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testAccuracy = multiclassEval.evaluate(bestModel.transform(testData))\n",
    "println(testAccuracy)\n",
    "\n",
    "val trainAccuracy = multiclassEval.evaluate(bestModel.transform(trainData))\n",
    "println(trainAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## udf 로 범주형 피쳐 다르게 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unhotUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5269/0x000000084180c040@2758d837,DoubleType,List(Some(class[value[0]: vector])),None,false,true)\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unhotUDF = udf((vec: Vector) => vec.toArray.indexOf(1.0).toDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wildernessCols: Array[String] = Array(Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3)\n",
       "wildernessAssembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_3bc447f94e7c, handleInvalid=error, numInputCols=4\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wildernessCols = (0 until 4).map(i => s\"Wilderness_Area_$i\").toArray\n",
    "\n",
    "val wildernessAssembler = new VectorAssembler().\n",
    "  setInputCols(wildernessCols).\n",
    "  setOutputCol(\"wilderness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wilderness: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 56 more fields]\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wilderness = wildernessAssembler.transform(assembledTrainData).withColumn(\"abc\", unhotUDF($\"wilderness\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Array[org.apache.spark.sql.Row] = Array([3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0], [3.0])\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilderness.select(\"abc\").head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|abc| count|\n",
      "+---+------+\n",
      "|0.0|234948|\n",
      "|1.0| 26853|\n",
      "|3.0| 33204|\n",
      "|2.0|228069|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wilderness.groupBy(\"abc\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shuffledDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 56 more fields]\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val shuffledDF = wilderness.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: Array[org.apache.spark.sql.Row] = Array([0.0], [2.0], [1.0], [2.0], [2.0], [2.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [2.0], [0.0], [2.0], [3.0], [2.0], [2.0], [0.0], [2.0], [0.0], [0.0], [2.0], [0.0], [0.0], [0.0], [0.0], [2.0], [0.0], [3.0], [2.0], [0.0], [2.0], [1.0], [2.0], [0.0], [2.0], [2.0], [2.0], [1.0], [0.0], [2.0], [2.0], [0.0], [1.0], [0.0], [0.0], [2.0])\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffledDF.select(\"abc\").head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Array[org.apache.spark.sql.Row] = Array([0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0])\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildernessAssembler.transform(assembledTrainData).select(\"Wilderness_Area_0\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res44: Array[org.apache.spark.sql.Row] = Array([0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0])\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildernessAssembler.transform(assembledTrainData).select(\"Wilderness_Area_1\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: Array[org.apache.spark.sql.Row] = Array([0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0])\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildernessAssembler.transform(assembledTrainData).select(\"Wilderness_Area_2\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res46: Array[org.apache.spark.sql.Row] = Array([1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1])\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildernessAssembler.transform(assembledTrainData).select(\"Wilderness_Area_3\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unencodeOneHot: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unencodeOneHot(data: DataFrame): DataFrame = {\n",
    "    val wildernessCols = (0 until 4).map(i => s\"Wilderness_Area_$i\").toArray\n",
    "\n",
    "    val wildernessAssembler = new VectorAssembler().\n",
    "      setInputCols(wildernessCols).\n",
    "      setOutputCol(\"wilderness\")\n",
    "\n",
    "    val unhotUDF = udf((vec: Vector) => vec.toArray.indexOf(1.0).toDouble)\n",
    "\n",
    "    val withWilderness = wildernessAssembler.transform(data).\n",
    "      drop(wildernessCols:_*).\n",
    "      withColumn(\"wilderness\", unhotUDF($\"wilderness\"))\n",
    "\n",
    "    val soilCols = (0 until 40).map(i => s\"Soil_Type_$i\").toArray\n",
    "\n",
    "    val soilAssembler = new VectorAssembler().\n",
    "      setInputCols(soilCols).\n",
    "      setOutputCol(\"soil\")\n",
    "\n",
    "    soilAssembler.transform(withWilderness).\n",
    "      drop(soilCols:_*).\n",
    "      withColumn(\"soil\", unhotUDF($\"soil\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unencTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unencTrainData = unencodeOneHot(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, wilderness, soil)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_68f197e89966, handleInvalid=error, numInputCols=12\n",
       "indexer: org.apache.spark.ml.feature.VectorIndexer = vecIdx_67d0ee63ca7b\n",
       "classifier: org.apache.spark.ml.classification.RandomForestClassifier = rfc_1e23e171732d\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_8dc1bf5baba2\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputCols = unencTrainData.columns.filter(_ != \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "  setInputCols(inputCols).\n",
    "  setOutputCol(\"featureVector\")\n",
    "\n",
    "val indexer = new VectorIndexer().\n",
    "  setMaxCategories(40).\n",
    "  setInputCol(\"featureVector\").\n",
    "  setOutputCol(\"indexedVector\")\n",
    "\n",
    "val classifier = new RandomForestClassifier().\n",
    "  setSeed(Random.nextLong()).\n",
    "  setLabelCol(\"Cover_Type\").\n",
    "  setFeaturesCol(\"indexedVector\").\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setImpurity(\"entropy\").\n",
    "  setMaxDepth(20).\n",
    "  setMaxBins(300)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier: org.apache.spark.ml.classification.RandomForestClassifier = rfc_f0ef07a4ed84\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifier = new RandomForestClassifier().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setLabelCol(\"Cover_Type\").\n",
    "      setFeaturesCol(\"indexedVector\").\n",
    "      setPredictionCol(\"prediction\").\n",
    "      setImpurity(\"entropy\").\n",
    "      setMaxDepth(20).\n",
    "      setMaxBins(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft mmlspark LGBM\n",
    "https://github.com/Azure/mmlspark/blob/master/docs/lightgbm.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
