{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://912db3bbf5c4:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1610966521758)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import scala.collection.Map\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import scala.util.Random\n",
       "import org.apache.spark.broadcast.Broadcast\n",
       "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           1988        1023          69         398         894         444\n",
      "Swap:          1023          23        1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2aac92f0\n",
       "res0: Array[(String, String)] = Array((spark.driver.extraJavaOptions,\"-Dio.netty.tryReflectionSetAccessible=true\"), (spark.executor.id,driver), (spark.repl.class.uri,spark://912db3bbf5c4:46139/classes), (spark.driver.port,46139), (spark.driver.host,912db3bbf5c4), (spark.app.name,spylon-kernel), (spark.executor.extraJavaOptions,\"-Dio.netty.tryReflectionSetAccessible=true\"), (spark.rdd.compress,True), (spark.repl.class.outputDir,/tmp/tmpxjqlfwxg), (spark.app.id,local-1610966521758), (spark.serializer.objectStreamReset,100), (spark.master,local[*]), (spark.submit.pyFiles,\"\"), (spark.submit.deployMode,client), (spark.ui.showConsoleProgress,true))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().config(\"spark.executor.memory\", \"4g\").getOrCreate()\n",
    "spark.sparkContext.getConf.getAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_path: String = /home/jovyan/work/ch03/data/\n",
       "user_artist_path: String = user_artist_data.txt\n",
       "artist_path: String = artist_data.txt\n",
       "alias_path: String = artist_alias.txt\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val base_path = \"/home/jovyan/work/ch03/data/\"\n",
    "val user_artist_path = \"user_artist_data.txt\"\n",
    "val artist_path = \"artist_data.txt\"\n",
    "val alias_path = \"artist_alias.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000002 1 55\n",
      "1000002 1000006 33\n",
      "1000002 1000007 8\n",
      "1000002 1000009 144\n",
      "1000002 1000010 314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawUserArtistData: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawUserArtistData = spark.read.textFile(base_path + user_artist_path)\n",
    "rawUserArtistData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.Dataset[(Int, Int)] = [_1: int, _2: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserArtistData.map{ line =>\n",
    "    val Array(user, artist, _*) = line.split(' ')\n",
    "    (user.toInt, artist.toInt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24296858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "userArtistDF: org.apache.spark.sql.DataFrame = [user: int, artist: int]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userArtistDF = rawUserArtistData.map{ line =>\n",
    "    val Array(user, artist, _*) = line.split(' ')  // _도 괜찮던데?\n",
    "    (user.toInt, artist.toInt)\n",
    "}.toDF(\"user\", \"artist\")\n",
    "\n",
    "println(userArtistDF.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.sql.Row] = Array([1000002,1], [1000002,1000006], [1000002,1000007])\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userArtistDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, artist: int]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userArtistDF.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   user| artist|\n",
      "+-------+-------+\n",
      "|1000002|      1|\n",
      "|1000002|1000006|\n",
      "|1000002|1000007|\n",
      "+-------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[org.apache.spark.sql.Row] = Array([1000002,1], [1000002,1000006], [1000002,1000007])\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userArtistDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------+\n",
      "|min(user)|max(user)|min(artist)|max(artist)|\n",
      "+---------+---------+-----------+-----------+\n",
      "|       90|  2443548|          1|   10794401|\n",
      "+---------+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.agg(\n",
    "    min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------+\n",
      "|min(user)|max(user)|min(artist)|max(artist)|\n",
      "+---------+---------+-----------+-----------+\n",
      "|    10015|  1009847|          1|   10788645|\n",
      "+---------+---------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, artist: int]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val b = userArtistDF.limit(1000000)\n",
    "b.agg(\n",
    "    min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134999\t06Crazy Life\n",
      "6821360\tPang Nakarin\n",
      "10113088\tTerfel, Bartoli- Mozart: Don\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawArtistData: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawArtistData = spark.read.textFile(base_path + artist_path)\n",
    "rawArtistData.take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "rawArtistData.map { line =>\n",
    "    val (id, name) = line.span(_ != '\\t')\n",
    "    (id.toInt, name.trim)\n",
    "}.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1134999,06Crazy Life]\n",
      "[6821360,Pang Nakarin]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "artistById: org.apache.spark.sql.DataFrame = [id: int, name: string]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val artistById = rawArtistData.flatMap { line =>\n",
    "    val (id, name) = line.span(_ != '\\t')\n",
    "    if (name.isEmpty) {\n",
    "        None\n",
    "    } else {\n",
    "        try {\n",
    "            Some((id.toInt), name.trim)\n",
    "        } catch {\n",
    "            case _: NumberFormatException => None\n",
    "        }\n",
    "    }\n",
    "}.toDF(\"id\", \"name\")\n",
    "\n",
    "artistById.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092764\t1000311\n",
      "1095122\t1000557\n",
      "6708070\t1007267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawArtistAlias: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawArtistAlias = spark.read.textFile(base_path + alias_path)\n",
    "rawArtistAlias.take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artistAlias: scala.collection.immutable.Map[Int,Int] = Map(1208690 -> 1003926, 2012757 -> 4569, 6949139 -> 1085752, 1109727 -> 1239120, 6772751 -> 1244705, 2070533 -> 1021544, 1157679 -> 2194, 9969617 -> 5630, 2034496 -> 1116214, 6764342 -> 40, 1272489 -> 1278238, 2108744 -> 1009267, 10349857 -> 1000052, 2145319 -> 1020463, 2126338 -> 2717, 10165456 -> 1001169, 6779368 -> 1239506, 10278137 -> 1001523, 9939075 -> 1329390, 2037201 -> 1274155, 1248585 -> 2885, 1106945 -> 1399, 6811322 -> 1019016, 9978396 -> 1784, 6676961 -> 1086433, 2117821 -> 2611, 6863616 -> 1277013, 6895480 -> 1000993, 6831632 -> 1246136, 1001719 -> 1009727, 10135633 -> 4250, 7029291 -> 1034635, 6967939 -> 1002734, 6864694 -> 1017311, 1237279 -> 1029752, 6793956 -> 1283231, 1208609 -> 1000699, 6693428 -> 1100258, 685174...\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val artistAlias = rawArtistAlias.flatMap{ line =>\n",
    "    val Array(artist, alias) = line.split('\\t')\n",
    "    if (artist.isEmpty) {\n",
    "        None\n",
    "    } else {\n",
    "        Some((artist.toInt, alias.toInt))\n",
    "    }\n",
    "}.collect().toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "artistById.filter($\"id\" isin (1208690, 1003926)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Int = 0\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artistAlias.getOrElse(325235133, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildCounts: (rawUserArtistData: org.apache.spark.sql.Dataset[String], bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[Int,Int]])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def buildCounts(\n",
    "    rawUserArtistData: Dataset[String],\n",
    "    bArtistAlias: Broadcast[scala.collection.immutable.Map[Int, Int]]\n",
    ") : DataFrame = {\n",
    "    rawUserArtistData.map { line =>\n",
    "        val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "        val finalAritstID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "        (userID, finalAritstID, count)\n",
    "    }.toDF(\"user\", \"artist\", \"count\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleUserArtistData: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleUserArtistData = rawUserArtistData.limit(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val bArtistAlias = spark.sparkContext.broadcast(artistAlias)\n",
    "val trainData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[Int,Int]] = Broadcast(25)\n",
       "trainData: org.apache.spark.sql.DataFrame = [user: int, artist: int ... 1 more field]\n",
       "res14: trainData.type = [user: int, artist: int ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bArtistAlias = spark.sparkContext.broadcast(artistAlias)\n",
    "val trainData = buildCounts(sampleUserArtistData, bArtistAlias)\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000002,1,55]\n"
     ]
    }
   ],
   "source": [
    "trainData.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.recommendation.ALSModel = ALSModel: uid=als_766fde51ac9c, rank=10\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = new ALS().\n",
    "  setSeed(Random.nextLong()).\n",
    "  setImplicitPrefs(true).\n",
    "  setRank(10).\n",
    "  setRegParam(0.01).\n",
    "  setAlpha(1.0).\n",
    "  setMaxIter(5).\n",
    "  setUserCol(\"user\").\n",
    "  setItemCol(\"artist\").\n",
    "  setRatingCol(\"count\").\n",
    "  setPredictionCol(\"prediction\").\n",
    "  fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id     |features                                                                                                                             |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1000020|[-0.09962447, -0.013246033, 0.5240463, -0.08857838, 0.56820846, -0.6588475, -0.149682, -0.8199164, -1.3176633, -0.17163086]          |\n",
      "|1000030|[-0.0069780033, 0.022746379, -0.02518704, -0.026718441, 0.04769788, 0.025246091, 0.06673875, 0.013707696, -0.057255972, -0.035054445]|\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.userFactors.show(2, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|     id|                name|\n",
      "+-------+--------------------+\n",
      "|1248399|   Bjrk & Thom Yorke|\n",
      "|1251278|     Love Of Lesbian|\n",
      "|1004461|          Gary Jules|\n",
      "|1252420|   The Ordinary Boys|\n",
      "|1262123|Valley of the Giants|\n",
      "|1268554|Elvis Presley vs....|\n",
      "|   5496| Echo & the Bunnymen|\n",
      "|1001891|Elvis Costello & ...|\n",
      "|   1195|      Elvis Costello|\n",
      "|1281772|       King Creosote|\n",
      "|1004513|     Daniel Johnston|\n",
      "|   5795|            Manitoba|\n",
      "|   4985|            Hangedup|\n",
      "|   1870|           808 State|\n",
      "|1001265|            Elastica|\n",
      "|1014826|       Junior Senior|\n",
      "|   5687|The Jimi Hendrix ...|\n",
      "|1001286|                Gene|\n",
      "|1000848|          Roxy Music|\n",
      "|1029443|Meanwhile, Back I...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "userID: Int = 1000020\n",
       "existingArtisdIDs: Array[Int] = Array(1, 1000024, 1000028, 1000044, 1000054, 1000062, 1000105, 1000113, 1000127, 1000130, 1000139, 1000188, 1000230, 1000233, 1000283, 1000289, 1000313, 1000315, 1000323, 1000333, 1000343, 1000388, 1000421, 1000429, 1000462, 1000505, 1000522, 1000557, 1000569, 1000591, 1000631, 1000660, 1000665, 1000676, 1000722, 1000724, 1000846, 1000848, 1000860, 1000864, 1000886, 1000930, 1000977, 1000985, 1001002, 15, 1001062, 1001072, 1001134, 1001145, 1001148, 1001152, 1001169, 1001265, 1001278, 1001286, 1001301, 1001306, 1001363, 1001383, 1001428, 1001435, 1001443, 1001449, 1001450, 1001459, 1001514, 1001525, 1001531, 1001588, 1001592, 1001646, 1001713, 1001714, 1001727, 1001735, 1001741, 1001762, 1001770, 1001779, 1001822, 1001864, 1268554, 1...\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userID = 1000020\n",
    "val existingArtisdIDs = trainData.filter($\"user\" === userID).select(\"artist\").as[Int].collect()\n",
    "artistById.filter($\"id\" isin (existingArtisdIDs:_*)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeRecommendations: (model: org.apache.spark.ml.recommendation.ALSModel, userID: Int, howMany: Int)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeRecommendations(\n",
    "    model: ALSModel,\n",
    "    userID: Int,\n",
    "    howMany: Int): DataFrame = {\n",
    "    \n",
    "    val toRecommend = model.itemFactors.select($\"id\".as(\"artist\")).withColumn(\"user\", lit(userID))\n",
    "    model.transform(toRecommend).select(\"artist\", \"prediction\").orderBy($\"prediction\".desc).limit(howMany)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|artist|   user|\n",
      "+------+-------+\n",
      "|    30|1000020|\n",
      "|    40|1000020|\n",
      "|    70|1000020|\n",
      "+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "toRecommend: org.apache.spark.sql.DataFrame = [artist: int, user: int]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val toRecommend = model.itemFactors.select($\"id\".as(\"artist\")).withColumn(\"user\", lit(1000020))\n",
    "toRecommend.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.sql.Column = 100\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "| artist|prediction|\n",
      "+-------+----------+\n",
      "|   1235| 1.1371856|\n",
      "|   1187| 1.1039873|\n",
      "|   1262| 1.0945467|\n",
      "|    232| 1.0836058|\n",
      "|1034635| 1.0827067|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topRecommendations: org.apache.spark.sql.DataFrame = [artist: int, prediction: float]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topRecommendations = makeRecommendations(model, userID, 5)\n",
    "topRecommendations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|     id|                name|\n",
      "+-------+--------------------+\n",
      "|1034635|           [unknown]|\n",
      "|    232|                Pulp|\n",
      "|   1262|           PJ Harvey|\n",
      "|   1235|Kings of Convenience|\n",
      "|   1187|               Doves|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "recoArtistIDs: Array[Int] = Array(1235, 1187, 1262, 232, 1034635)\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recoArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "artistById.filter($\"id\" isin (recoArtistIDs:_*)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **AUC, Hyperparemeter 제외**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "areaUnderCurve: (positiveData: org.apache.spark.sql.DataFrame, bAllArtistIDs: org.apache.spark.broadcast.Broadcast[Array[Int]], predictFunction: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame)Double\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def areaUnderCurve(\n",
    "      positiveData: DataFrame,\n",
    "      bAllArtistIDs: Broadcast[Array[Int]],\n",
    "      predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "    // What this actually computes is AUC, per user. The result is actually something\n",
    "    // that might be called \"mean AUC\".\n",
    "\n",
    "    // Take held-out data as the \"positive\".\n",
    "    // Make predictions for each of them, including a numeric score\n",
    "    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "      withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "        // Make at most one pass over all artists to avoid an infinite loop.\n",
    "        // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "          // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "        // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    // Join positive predictions to negative predictions by user, only.\n",
    "    // This will result in a row for every possible pairing of positive and negative\n",
    "    // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "    // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "    // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, \"user\").\n",
    "      select($\"user\", ($\"correct\" / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allData: org.apache.spark.sql.DataFrame = [user: int, artist: int ... 1 more field]\n",
       "someUsers: Array[Int] = Array(1000190, 1001043, 1001129, 1001139, 1002431, 1002605, 1004666, 1005158, 1005439, 1005697)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "val someUsers = allData.select(\"user\").as[Int].distinct().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000190 -> 6694932, 1006411, 1233321, 420, 1006354\n",
      "1001043 -> 1854, 393, 1006016, 930, 1034635\n",
      "1001129 -> 393, 1034635, 15, 59, 979\n",
      "1001139 -> 393, 59, 1034635, 15, 121\n",
      "1002431 -> 4629, 3327, 1001646, 1274, 1000062\n",
      "1002605 -> 393, 4629, 1034635, 930, 1274\n",
      "1004666 -> 1000442, 1058104, 4629, 1002560, 1034635\n",
      "1005158 -> 1854, 4468, 1006016, 4629, 1000052\n",
      "1005439 -> 1000183, 1001384, 1000152, 1000630, 1002280\n",
      "1005697 -> 1002065, 2814, 1854, 930, 1177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "someRecommendations: Array[(Int, org.apache.spark.sql.DataFrame)] = Array((1000190,[artist: int, prediction: float]), (1001043,[artist: int, prediction: float]), (1001129,[artist: int, prediction: float]), (1001139,[artist: int, prediction: float]), (1002431,[artist: int, prediction: float]), (1002605,[artist: int, prediction: float]), (1004666,[artist: int, prediction: float]), (1005158,[artist: int, prediction: float]), (1005439,[artist: int, prediction: float]), (1005697,[artist: int, prediction: float]))\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val someRecommendations = someUsers.map(userID => (userID, makeRecommendations(model, userID, 5)))\n",
    "someRecommendations.foreach { case (userID, recsDF) => \n",
    "    val recommendedArtist = recsDF.select(\"artist\").as[Int].collect()\n",
    "    println(s\"$userID -> ${recommendedArtist.mkString(\", \")}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunRecommender(private val spark: SparkSession) {\n",
    "\n",
    "\n",
    "  def model(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      rawArtistData: Dataset[String],\n",
    "      rawArtistAlias: Dataset[String]): Unit = {\n",
    "\n",
    "    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "    val trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "\n",
    "    val model = new ALS().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setImplicitPrefs(true).\n",
    "      setRank(10).\n",
    "      setRegParam(0.01).\n",
    "      setAlpha(1.0).\n",
    "      setMaxIter(5).\n",
    "      setUserCol(\"user\").\n",
    "      setItemCol(\"artist\").\n",
    "      setRatingCol(\"count\").\n",
    "      setPredictionCol(\"prediction\").\n",
    "      fit(trainData)\n",
    "\n",
    "    trainData.unpersist()\n",
    "\n",
    "    model.userFactors.select(\"features\").show(truncate = false)\n",
    "\n",
    "    val userID = 2093760\n",
    "\n",
    "    val existingArtistIDs = trainData.\n",
    "      filter($\"user\" === userID).\n",
    "      select(\"artist\").as[Int].collect()\n",
    "\n",
    "    val artistByID = buildArtistByID(rawArtistData)\n",
    "\n",
    "    artistByID.filter($\"id\" isin (existingArtistIDs:_*)).show()\n",
    "\n",
    "    val topRecommendations = makeRecommendations(model, userID, 5)\n",
    "    topRecommendations.show()\n",
    "\n",
    "    val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "\n",
    "    artistByID.filter($\"id\" isin (recommendedArtistIDs:_*)).show()\n",
    "\n",
    "    model.userFactors.unpersist()\n",
    "    model.itemFactors.unpersist()\n",
    "  }\n",
    "\n",
    "  def evaluate(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      rawArtistAlias: Dataset[String]): Unit = {\n",
    "\n",
    "    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "    val allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "    val Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1))\n",
    "    trainData.cache()\n",
    "    cvData.cache()\n",
    "\n",
    "    val allArtistIDs = allData.select(\"artist\").as[Int].distinct().collect()\n",
    "    val bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)\n",
    "\n",
    "    val mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\n",
    "    println(mostListenedAUC)\n",
    "\n",
    "    val evaluations =\n",
    "      for (rank     <- Seq(5,  30);\n",
    "           regParam <- Seq(1.0, 0.0001);\n",
    "           alpha    <- Seq(1.0, 40.0))\n",
    "      yield {\n",
    "        val model = new ALS().\n",
    "          setSeed(Random.nextLong()).\n",
    "          setImplicitPrefs(true).\n",
    "          setRank(rank).setRegParam(regParam).\n",
    "          setAlpha(alpha).setMaxIter(20).\n",
    "          setUserCol(\"user\").setItemCol(\"artist\").\n",
    "          setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "          fit(trainData)\n",
    "\n",
    "        val auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\n",
    "\n",
    "        model.userFactors.unpersist()\n",
    "        model.itemFactors.unpersist()\n",
    "\n",
    "        (auc, (rank, regParam, alpha))\n",
    "      }\n",
    "\n",
    "    evaluations.sorted.reverse.foreach(println)\n",
    "\n",
    "    trainData.unpersist()\n",
    "    cvData.unpersist()\n",
    "  }\n",
    "\n",
    "  def recommend(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      rawArtistData: Dataset[String],\n",
    "      rawArtistAlias: Dataset[String]): Unit = {\n",
    "\n",
    "    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "    val allData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "    val model = new ALS().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setImplicitPrefs(true).\n",
    "      setRank(10).setRegParam(1.0).setAlpha(40.0).setMaxIter(20).\n",
    "      setUserCol(\"user\").setItemCol(\"artist\").\n",
    "      setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "      fit(allData)\n",
    "    allData.unpersist()\n",
    "\n",
    "    val userID = 2093760\n",
    "    val topRecommendations = makeRecommendations(model, userID, 5)\n",
    "\n",
    "    val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "    val artistByID = buildArtistByID(rawArtistData)\n",
    "    artistByID.join(spark.createDataset(recommendedArtistIDs).toDF(\"id\"), \"id\").\n",
    "      select(\"name\").show()\n",
    "\n",
    "    model.userFactors.unpersist()\n",
    "    model.itemFactors.unpersist()\n",
    "  }\n",
    "\n",
    "  def buildArtistByID(rawArtistData: Dataset[String]): DataFrame = {\n",
    "    rawArtistData.flatMap { line =>\n",
    "      val (id, name) = line.span(_ != '\\t')\n",
    "      if (name.isEmpty) {\n",
    "        None\n",
    "      } else {\n",
    "        try {\n",
    "          Some((id.toInt, name.trim))\n",
    "        } catch {\n",
    "          case _: NumberFormatException => None\n",
    "        }\n",
    "      }\n",
    "    }.toDF(\"id\", \"name\")\n",
    "  }\n",
    "\n",
    "  def buildArtistAlias(rawArtistAlias: Dataset[String]): Map[Int,Int] = {\n",
    "    rawArtistAlias.flatMap { line =>\n",
    "      val Array(artist, alias) = line.split('\\t')\n",
    "      if (artist.isEmpty) {\n",
    "        None\n",
    "      } else {\n",
    "        Some((artist.toInt, alias.toInt))\n",
    "      }\n",
    "    }.collect().toMap\n",
    "  }\n",
    "\n",
    "  def buildCounts(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "    rawUserArtistData.map { line =>\n",
    "      val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "      val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "      (userID, finalArtistID, count)\n",
    "    }.toDF(\"user\", \"artist\", \"count\")\n",
    "  }\n",
    "\n",
    "  def makeRecommendations(model: ALSModel, userID: Int, howMany: Int): DataFrame = {\n",
    "    val toRecommend = model.itemFactors.\n",
    "      select($\"id\".as(\"artist\")).\n",
    "      withColumn(\"user\", lit(userID))\n",
    "    model.transform(toRecommend).\n",
    "      select(\"artist\", \"prediction\").\n",
    "      orderBy($\"prediction\".desc).\n",
    "      limit(howMany)\n",
    "  }\n",
    "\n",
    "  def areaUnderCurve(\n",
    "      positiveData: DataFrame,\n",
    "      bAllArtistIDs: Broadcast[Array[Int]],\n",
    "      predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "    // What this actually computes is AUC, per user. The result is actually something\n",
    "    // that might be called \"mean AUC\".\n",
    "\n",
    "    // Take held-out data as the \"positive\".\n",
    "    // Make predictions for each of them, including a numeric score\n",
    "    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "      withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "        // Make at most one pass over all artists to avoid an infinite loop.\n",
    "        // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "          // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "        // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    // Join positive predictions to negative predictions by user, only.\n",
    "    // This will result in a row for every possible pairing of positive and negative\n",
    "    // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "    // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "    // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, \"user\").\n",
    "      select($\"user\", ($\"correct\" / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "  }\n",
    "\n",
    "  def predictMostListened(train: DataFrame)(allData: DataFrame): DataFrame = {\n",
    "    val listenCounts = train.groupBy(\"artist\").\n",
    "      agg(sum(\"count\").as(\"prediction\")).\n",
    "      select(\"artist\", \"prediction\")\n",
    "    allData.\n",
    "      join(listenCounts, Seq(\"artist\"), \"left_outer\").\n",
    "      select(\"user\", \"artist\", \"prediction\")\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object RunRecommender {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val spark = SparkSession.builder().getOrCreate()\n",
    "    // Optional, but may help avoid errors due to long lineage\n",
    "    spark.sparkContext.setCheckpointDir(\"hdfs:///tmp/\")\n",
    "\n",
    "    val base = \"hdfs:///user/ds/\"\n",
    "    val rawUserArtistData = spark.read.textFile(base + \"user_artist_data.txt\")\n",
    "    val rawArtistData = spark.read.textFile(base + \"artist_data.txt\")\n",
    "    val rawArtistAlias = spark.read.textFile(base + \"artist_alias.txt\")\n",
    "\n",
    "    val runRecommender = new RunRecommender(spark)\n",
    "    runRecommender.preparation(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "    runRecommender.model(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "    runRecommender.evaluate(rawUserArtistData, rawArtistAlias)\n",
    "    runRecommender.recommend(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "  }\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
